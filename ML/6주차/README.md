# ML 스터디 6주차 : Gradient Descent Alogrithm

## **정초이**

## 1. Cost functions

### ***선형 회귀***

결과값 (output value)을 결정할 것이라고 추정되는 입력값 (input value)과 결과 값의 연관관계를 찾고, 이를 선형 관계를 통해 찾는 방법이 선형 회귀 (Linear regression).

### ***택시 요금 예시로 이해하기***

![map](./img/gradient/1.png)

택시 요금 : 일반적으로 거리에 비례해서 요금 부과.

—> 결과값 (요금)과 입력값 (거리)의 관계를 찾아야 한다!



<img src="./img/gradient/2.png" width=400>



거리별 요금을 그래프로 그리고, 원본 데이터의 거리를 x_data, 거리에 따라 측정된 택시 요금을 y_origin 이라고 하자.



#### 가설 정의하기

거리와 요금이 서로 비례하기 때문에, 

거리(x_data)와 요금(y_data)간의 상관 관계는 다음과 같이 일차 방정식과 형태의 그래프를 그리게 된다고 가정하자.

>  W (Weight): 그래프의 각도, b:  bias

#### y_data = Wx_data + b

이 일차 방정식 형태로 대충 1차원 그래프를 그려보면 같은 형태로 아래와 같이 그래프를 그릴 수 있다.

<img src="./img/gradient/3.png" width=400>

그래프의 기울기가 맞지 않는 것 같아 그래프의 각도와 높이를 아래와 같이 보정할 수도 있다.

<img src="./img/gradient/4.png" width=400>

그래프를 보정해도 각도와 높이가 맞지 않는 것 같을 때, 각도 W와 높이 b의 값은 어떻게 찾을 수 있을까?

이때 사용하는 것이 **Cost Function**이다.



### ***Cost Function***

우리가 구하고자 하는 그래프는 실제 값(빨간 점들)에서 그래프(파란 선위의 점들)의 값까지 **차이가 가장 작은 값**을 구하고자 하는 것이다. 

아래와 같이 **y_data=W(x_data) +b**와 같은 그래프를 그렸다고 하자.

<img src="./img/gradient/5.png" width=400>

여기서 실제 값과 그래프의 값의 차이는 `측정값-그래프의 값` 인데, 이를 d라고 하자.

이를 변수 이름을 사용해 나타내면 ` d = y_data - y_origin` 이 된다.

> y_origin : x_data에 따라 계산된 실제 요금 값, y_data: 파란색 그래프 위의 값

이때 d는 한개가 아니라 n개이기 때문에, `dn = y_data_n - y_origin_n` 으로 나타낼 수 있다.



```
우리가 구하고자 하는 값

= dn의 합이 최소가 되는 W와 b의 값

= 실제 측정한값과, 예측한 값의 차이가 최소가 되는 W와 b
```



dn은 위의 그래프에서 처럼 그래프 위에도 있을 수 있지만 (dn이 양수일 때), 그래프 아래에도 있을 수 있다, (dn이 음수일 때). 

합을 구하면, 예측 선에서의 실측값 까지의 거리의 합이 되지 않기 때문에, dn에 대한 제곱을 사용하자. (절댓값을 사용하기도 함)

<img src="./img/gradient/6.png">

> 평균을 사용하기 위해 데이터의 개수만큼 나눠주기도 하고, 1/2를 해주기도 하는 것 같다.
>
> 1/2를 해주는 이유는 나중에 신경망 학습에서 W의 gradient를 구할 때 계산을 간단히 하기 위함이다.

위와 같이 모든 오차의 값의 제곱을 합하고 데이터의 개수로 나누면 오차의 제곱의 평균이 된다.

오차를 가장 적게 만드려면 위 함수의 값을 최소화해야 하며, 이 함수가 **비용함수** *Cost(W,b)*이다.





### ***비용함수의 종류***

### Sum of squared error

<img src="./img/gradient/6.png">

- Linear regression 에서 많이 사용함.
  - 2차 함수처럼 표현 가능 —> 오차가 클 수록 cost 값이 비례하여 커짐

### Sum of squared error의 학습 규칙

미니배치 / 모든 x 전체를 한번 돌 때 마다 (1 epoch)

**각 가중치 <- 각 가중치 - eta \* ( sum( ( cost미분 * 활성함수_미분 ) * x ) / n )**



eta = learning rate

n = x의 개수

cost미분 = p - y

>  여기서 cost미분을 error, (cost미분 * 활성함수_미분)을 delta 라고 부른다.



ex)

활성함수가 f(x) = x 인 경우, delta = error.

활성함수가 sigmoid일 경우,

sigmoid미분 = sigmoid(z) * (1-sigmoid(z)) 이므로,

**delta = (p - y) \* p * (1 - p)**



### Cross Entropy

<img src="./img/gradient/9.png">

- Logistic regression이나, 분류나, 딥러닝에서 많이 사용.
  - Logistic regression에서 시그모이드 함수를 적용시키면
    H(x)에 지수함수가 포함되어 있는 모양 —> cost 값이 비례하지는 않음.
  - Cost 최소 지점을 찾기 위해 gradient descent algorithm을 사용하는데 sigmoid를 적용시키면
    Sum of squared error는 Local minimum에서 멈출 수 있음 —> Cross Entropy 사용.
- 수렴이 빨라서 Linear regression이 아니라면 cross entropy를 추천한다고 한다.



- y = 1인 경우
  - H(x)가 0이면 cost는 무한대로 급격하게 커진다.
  - H(x)가 1이면 오차는 0이고 cost함수의 값도 0이다.

- y = 0 인 경우
  - H(x)가 1이면 cost는 무한대로 급격하게 커진다.
  - H(x)가 0이면 오차는 0이고 cost함수의 값도 0이다.

이것을 하나의 식으로 다음과 같이 나타낼 수 있다.

<img src="./img/gradient/8.png">

### ***Cross Entropy 의 학습 규칙***

**각 가중치 <- 각 가중치 - eta \* ( sum( ( delta ) * x ) / n )**



**delta = error = p - y**

※ SSE와의 차이는 delta에 sigmoid미분을 곱해주지 않는다는 점 뿐이다. 물론 이는 cross entropy loss function을 각 weight들에 대해 편미분해보면 알 수 있다. [링크](<https://www.ics.uci.edu/~pjsadows/notes.pdf>)를 참고하자.



***



##  2. Gradient Descent Algorithm

### ***개념***

<img src="./img/gradient/10.png">

> θ<sub>0</sub> = b,  θ<sub>1</sub>= W, J : cost function

x축은 b, y축은 W, z축은 비용함수의 결과값을 의미한다.

우리는 비용함수의 결과값이 최소가 되는 점을 찾아야 한다.

우리가 빨간색 부분(언덕 위의 x 표시)에 있다고 가정을 하고, 비용함수의 값이 최소가 되는 지점이 빨간 화살표가 가리키는 지점이라고 가정하자.

그렇다면 우리는 빨간 화살표가 가리키는 지점까지 최단 거리로 내려가야 하는데 그 길이 검정색 선으로 나타나 있다.



**빨간 언덕**: 첫 dataset을 투입했을 때, **검정 선**: 학습과정, **빨간 화살표**: 학습과정 끝에 설계한 가정 함수.



### ***함수***

<img src="./img/gradient/11.png">

> α: 빨간 언덕에서 내려올 때의 걸음 수. **한번에** 몇 걸음을 걷는가 —> α가 크면 빠르게 내려오는 것.
>
> α값 뒤의 미분값: 언덕의 경사, 기울기.
>
> 수식 := —> 우측 식을 계산한 결과값을 좌측 θ<sub>j</sub>로 대체한다는 뜻.

—> 현재 θ값에서 기울기*걸음 수 만큼 차감함을 의미한다.



### ***주의점***

<img src="./img/gradient/12.png">  

첫번째 그래프는 기울기가 양수인 경우를 나타낸 것이다. 이 경우에는 -α가 곱해져 θj값이 좌측으로 움직인다. 

두번째 그래프는 기울기가 음수인 경우를 나타낸 것이다. 이 경우에는 -α가 곱해져 θj 값이 우측으로 움직인다.

즉, **기울기가 양수인지, 음수인지와 관계없이 작동한다.**



### ***최종 함수***

<img src="./img/gradient/14.png">

위 식과 같이 미분값 * 비용함수를 풀어서 나타낼 수 있고, 

<img src="./img/gradient/15.png">

최종적으로 프로그래밍에 적용할 경사하강법의 함수는 위와 같이 나타낼 수 있다.



## 3. Learning rate

### ***Learning rate : 학습 계수***

#### α값

Cost Function에서 α값은, Learning rate 라고 하는 특별한 값이다.

- 너무 크지도, 너무 작지도 않은 값을 가져야 한다.
- 특정 가중치값 W(위에선 θ<sub>j</sub>)가 주어졌을 때 기울기에 α값, Learning rate 값을 곱해서 그 다음 가중치 값을 결정하게 된다.



<img src="./img/gradient/13.png"> 

첫번째 그래프는  α값이 너무 작은 경우를 나타낸 것이다. 여러 번 학습을 해야 최소점에 도달한다. 

아래 그래프는 α값이 너무 큰 경우를 나타낸 것이다. 바로 최소값으로 가지 못하고 지그재그 형태로 점점 올라간다.
이렇게 망한 학습이 되는 것을 Overshooting 이라고 한다. (W, θ<sub>j</sub> 값이 발산해 버린 것, 최소값을 지나쳐 버린 것)

그러므로, **적정한** **α값을 설정**해야 한다. 

> 그렇다고 α값을 지속적으로 수정해 줄 필요는 없다. 최소값으로 내려간 이후에는 기울기값이 0이 되므로 움직이지 않기 때문! 또한, 적절한 α값의 수치를 구하는 방법도 딱히 없다고 한다.  α값을 다양하게 잡아서 cost값이 감소하는 것을 관찰하며 수정하기!
> 적절한 α값은 굉장히 주관적인 말인 것이고, 경험이 중요한 것 같다 ……!



#### 적절한 α값

<img src="./img/gradient/16.png" width=500>

이상적으로 Gradient Descent Algorithm이 실행된다면 추적 순서는 이런 식으로 될 것이다!

## 4. Batch and Mini batch

### ***Batch Gradient Descent 배치 경사 하강법***

- Batch : 전체 데이터를 하나의 덩어리로 묶은 입력 데이터
- 전체 데이터 전체를 매 스텝에서 훈련 데이터로 사용한다. 
- 데이터셋이 커지면 속도가 아주 느려진다.
- 특성 수에 민감하지 않다.
- 수십만 개의 특성에서 선형 회귀를 훈련시키려면 경사 하강법을 사용하는 것이 훨씬 빠르다.

### ***Mini Batch Gradient Descent 미니 배치 경사 하강법***

- 확률적 경사 하강법 
  - 전체 데이터에서 1개만 뽑아 샘플화하여 훈련 데이터로 사용한다.
  - 전체 데이터에서 1개만 뽑기 때문에 빠르지만 신뢰도가 낮다.
- 미니 배치 경사 하강법
  - 배치 경사 하강법 + 확률적 경사 하강법
  - 전체 데이터에서 일부만 뽑아 batch를 만든다.
  - 그 후, 그 batch가 전체 데이터인 것 처럼 학습을 진행하며 weight 값을 수정한다. 그리고 이를 많이 반복한다.
- 인공신경망에서 일반적으로 batch라고 하면 이 mini-batch를 의미하는 것이라고 한다.

***

## **조민지**

